{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c1312b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing pipeline completed. Total time taken: 0.8789703845977783 seconds\n",
      "Training set shape: (7505, 25) (7505,)\n",
      "Testing set shape: (3217, 25) (3217,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data(api_url):\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Access the CSV data from the response\n",
    "        csv_data = response.content.decode('utf-8')\n",
    "        csv_io = StringIO(csv_data)\n",
    "\n",
    "        # Convert the CSV data to a Pandas DataFrame\n",
    "        df = pd.read_csv(csv_io)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code)\n",
    "        return None\n",
    "def preprocess_data(df):\n",
    "    # Filter the dataset for \"Flood\" incident type\n",
    "    filtered_df = df[df['incidentType'] == 'Flood']\n",
    "\n",
    "    # Drop columns with high null values\n",
    "    df = filtered_df.drop(['lastIAFilingDate', 'disasterCloseoutDate'], axis=1)\n",
    "\n",
    "    # Convert dates to datetime format\n",
    "    df['incidentBeginDate'] = pd.to_datetime(df['incidentBeginDate'])\n",
    "    df['incidentEndDate'] = pd.to_datetime(df['incidentEndDate'])\n",
    "    df['declarationDate'] = pd.to_datetime(df['declarationDate'])\n",
    "\n",
    "    # Forward fill missing values\n",
    "    df['incidentEndDate'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def derive_new_features(df):\n",
    "    # Derive new features\n",
    "    df['Duration_of_Incident'] = (df['incidentEndDate'] - df['incidentBeginDate']).dt.days\n",
    "    df['Declared_Programs_Count'] = df[['ihProgramDeclared', 'iaProgramDeclared', 'paProgramDeclared', 'hmProgramDeclared']].sum(axis=1)\n",
    "    \n",
    "    # Sort the dataframe by state and declarationDate in ascending order\n",
    "    df = df.sort_values(by=['declarationDate'])\n",
    "    #Time Since Last Disaster each state\n",
    "    df['Time Since Last Disaster'] = df.groupby('state')['declarationDate'].diff().dt.days.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def standardization(df):\n",
    "    # Apply standardization\n",
    "    scaler = StandardScaler()\n",
    "    numerical_columns = ['disasterNumber', 'fyDeclared', 'ihProgramDeclared', 'iaProgramDeclared', 'paProgramDeclared',\n",
    "                         'hmProgramDeclared', 'tribalRequest', 'fipsStateCode', 'fipsCountyCode', 'placeCode', 'declarationRequestNumber']\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def perform_train_test_split(df, target_column, test_size=0.3, random_state=42):\n",
    "    # Split the dataset into features (X) and target variable (y)\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Split the dataset into training set and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def data_processing_pipeline(api_url, target_column):\n",
    "    # Start the timer for the entire pipeline\n",
    "    start_time = time.time()\n",
    "\n",
    "    # perform all the steps\n",
    "    df = load_data(api_url)\n",
    "    if df is not None:\n",
    "        df = preprocess_data(df)\n",
    "\n",
    "        df = derive_new_features(df)\n",
    "\n",
    "        df = standardization(df)\n",
    "        df.to_csv('disaster_declaration_processed.csv', index=False)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = perform_train_test_split(df, target_column)\n",
    "\n",
    "        # Calculate the total time taken for the pipeline\n",
    "        total_time = time.time() - start_time\n",
    "        print(\"Data processing pipeline completed. Total time taken:\", total_time, \"seconds\")\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        print(\"Error occurred while loading the data.\")\n",
    "\n",
    "        \n",
    "# Define the API URL for data retrieval\n",
    "api_url = \"https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries.csv\"\n",
    "\n",
    "target_column = 'incidentType'\n",
    "\n",
    "# Call the data_processing_pipeline function\n",
    "X_train, X_test, y_train, y_test = data_processing_pipeline(api_url, target_column)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Save all datasets as CSV files\n",
    "X_train.to_csv('disaster_X_train.csv', index=False)\n",
    "X_test.to_csv('disaster_X_test.csv', index=False)\n",
    "y_train.to_csv('disaster_y_train.csv', index=False)\n",
    "y_test.to_csv('disaster_y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5f005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634c933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
